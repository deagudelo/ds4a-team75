{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import folium\n",
    "import json\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "import nltk # imports the natural language toolkit\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "import string\n",
    "import plotly\n",
    "import plotly.express as px\n",
    "from nltk.stem import PorterStemmer \n",
    "from collections import Counter\n",
    "from nltk.util import ngrams\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "from nltk.data import load\n",
    "from nltk.stem import SnowballStemmer\n",
    "from string import punctuation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import unicodedata\n",
    "import re\n",
    "from nltk.tag import StanfordPOSTagger\n",
    "# Aquí obtenemos la lista de tokens en \"tokens\"\n",
    "tagger=\"/home/ec2-user/stanford-tagger-4.0.0/models/spanish-ud.tagger\"\n",
    "jar=\"/home/ec2-user/stanford-tagger-4.0.0/stanford-postagger.jar\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "df = pd.read_excel('Original_Data/Reporte Uraba2019_CAGMV1Est.xlsx')\n",
    "with open('./GeoData/munis.geojson', encoding='utf-8') as geo:\n",
    "    geojson = json.loads(geo.read())\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# df[\"RepairCode\"].value_counts()\n",
    "# null_rc = df[df[\"RepairCode\"].isnull()]\n",
    "# len(df[\"RepairCode\"])\n",
    "list(df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#priority_col_dict\n",
    "#Mejorar los colores, función para cada RGB\n",
    "df.Latitude = df.Latitude/1000000\n",
    "df.Longitude = df.Longitude/1000000\n",
    "df.Priority.unique()\n",
    "priority_colors = ['#%02x%02x%02x' % (255, 0+(i*30), 0) for i in range(len(df.Priority.unique()))]\n",
    "priority_colors = list(reversed(priority_colors))\n",
    "priority_col_dict = dict(zip(df.Priority.unique(),priority_colors[-1::-1]))\n",
    "\n",
    "#Center in Apartadó, Antioquia: (7.88299, -76.62587)\n",
    "antioquia_map2 = folium.Map(location=[7.88299, -76.62587],\n",
    "                        zoom_start=9,\n",
    "                        tiles=\"OpenStreetMap\")\n",
    "\n",
    "for i in range(0,len(df)):\n",
    "    marker = folium.CircleMarker(location=[ df[\"Latitude\"][i], df[\"Longitude\"][i] ],\n",
    "                                 radius=2,\n",
    "                                 color= priority_col_dict[df.Priority[i]],\n",
    "                                 fill=True)\n",
    "    marker.add_to(antioquia_map2)\n",
    "\n",
    "\n",
    "antioquia_map2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# df.town.unique()\n",
    "df['town_upper'] = df.town.apply(lambda x: str(x).upper())\n",
    "# df.town_lower.unique()\n",
    "\n",
    "dff = df.groupby('town_upper').mean().reset_index()\n",
    "dff\n",
    "\n",
    "px.choropleth_mapbox(dff,                          #Data\n",
    "        locations='town_upper',                    #Column containing the identifiers used in the GeoJSON file \n",
    "        color='Priority',                          #Column giving the color intensity of the region\n",
    "        geojson=geojson,                           #The GeoJSON file\n",
    "        featureidkey=\"properties.MPIO_CNMBR\",\n",
    "        zoom=5,                                    #Zoom\n",
    "        mapbox_style=\"carto-positron\",             #Mapbox style, for different maps you need a Mapbox account and a token\n",
    "        center={\"lat\": 7.88299, \"lon\": -76.62587}, #Center\n",
    "        color_continuous_scale=\"Viridis\",          #Color Scheme\n",
    "        opacity=0.5,                               #Opacity of the map\n",
    "        )    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"RepairCodeString\"] = df[\"RepairCode\"].apply(lambda x: str(x).upper())\n",
    "\n",
    "text = \" \".join(list(df[\"RepairCodeString\"].unique()))\n",
    "# text\n",
    "# eliminate irrelevant words\n",
    "irrelevant = [\"RAMALES\", \" DE \", \" EN \",  \" POR \", \" O \" , \" Y \", \"/\" ,\" PARA \", \"(\", \")\", \" - \", \" -\",  \"- \" ]\n",
    "\n",
    "for st in irrelevant:\n",
    "    text = text.replace(st,\" \")\n",
    "\n",
    "# eliminate verbs\n",
    "for wo in text.split(\" \"):\n",
    "    word = str(wo).strip()\n",
    "    \n",
    "    if word.endswith(\"AR\") or word.endswith(\"ER\") or word.endswith(\"IR\"):\n",
    "        text = text.replace(word,\" \")\n",
    "    else : print(word)\n",
    "# Create and generate a word cloud image:\n",
    "wordcloud = WordCloud().generate(text)\n",
    "\n",
    "# Display the generated image:\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "# df[\"RepairCodeString\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text analysis RepairCode\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First we extract a text and clean it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do that, remove stopwords, NAN, RAMAL, RAMALES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "es_stopwords = [str(x).upper() for x in stopwords.words(\"spanish\")] \n",
    "\n",
    "def remove_accents(input_str):\n",
    "    nfkd_form = unicodedata.normalize('NFKD', input_str)\n",
    "    return u\"\".join([c for c in nfkd_form if not unicodedata.combining(c)])\n",
    "\n",
    "es_stopwords_na = [remove_accents(x) for x in es_stopwords]\n",
    "es_stopwords_na.extend([\"NAN\", \"RAMAL\", \"RAMALES\"])\n",
    "\n",
    "def clean_text(text):\n",
    "    # Remove non alphabetic charactets using pattern = r\"[^\\w]\" seen in class\n",
    "    \n",
    "    pattern = r\"[^\\w]\"\n",
    "    ret = re.sub(pattern, \" \", text)\n",
    "    ret = remove_accents(ret)\n",
    "    for bad in es_stopwords_na:\n",
    "        \n",
    "        to_replace = \" \" + bad + \" \" if bad != \"NAN\" else bad\n",
    "        ret = ret.replace(to_replace, \" \")\n",
    "    return ret\n",
    "    \n",
    "# Create clean column    \n",
    "df[\"RepairCodeStringClean\"] = df[\"RepairCodeString\"].apply(clean_text)\n",
    "    \n",
    "    \n",
    "\n",
    "all_reviews_text = ' '.join(df[\"RepairCodeString\"])\n",
    "all_reviews_text = clean_text(all_reviews_text)\n",
    "\n",
    "print(all_reviews_text)\n",
    "\n",
    "\n",
    "# Get tokens\n",
    "tokenized_words = nltk.word_tokenize(all_reviews_text)\n",
    "# remove length smaller than 2\n",
    "tokenized_words = [each.strip() for each in tokenized_words if len(each.lower()) > 2]\n",
    "\n",
    "\n",
    "word_freq = Counter(tokenized_words)\n",
    "ten_pct =round(len(word_freq)*0.1)\n",
    "\n",
    "## Top 10%\n",
    "word_freq.most_common(ten_pct)\n",
    "\n",
    "## Similarly, bottom 10%\n",
    "word_freq.most_common()[-ten_pct:-1]\n",
    "\n",
    "df[\"RepairCodeStringClean\"].apply(lambda x: np.nan if str(x).strip() == \"\" else x).dropna().head()\n",
    "\n",
    "## First 5 repair codes n-grams\n",
    "# first_5_revs = AllRCs[0:5]\n",
    "# word_tokens = nltk.word_tokenize(''.join(first_5_revs))\n",
    "# list(ngrams(word_tokens, 3)) #ngrams(word_tokens,n) gives the n-grams."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# N-Grams RepairCode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_k_ngrams(word_tokens,n,k):\n",
    "    ## Getting them as n-grams\n",
    "    n_gram_list = list(ngrams(word_tokens, n))\n",
    "    ### Getting each n-gram as a separate string\n",
    "    n_gram_strings = [' '.join(each) for each in n_gram_list]\n",
    "    n_gram_counter = Counter(n_gram_strings)\n",
    "    most_common_k = n_gram_counter.most_common(k)\n",
    "    print(most_common_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k_ngrams(tokenized_words, 1, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k_ngrams(tokenized_words, 2, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "top_k_ngrams(tokenized_words, 3, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k_ngrams(tokenized_words, 4, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.pos_tag(tokenized_words)\n",
    "# import spaghetti as sgt\n",
    "\n",
    "# sent1 = 'Mi colega me ayuda a programar cosas .'.split()\n",
    "# sent2 = 'Está embarazada .'.split()\n",
    "# test_sents = [sent1, sent2]\n",
    "\n",
    "# # Default Spaghetti tagger.\n",
    "# print (sgt.pos_tag(test_sent))\n",
    "\n",
    "# # Tag multiple sentences.\n",
    "# print (sgt.pos_tag_sents(test_sents))\n",
    "\n",
    "# spa_tagger = sgt.CESSTagger()\n",
    "# # POS tagger trained on unigrams of CESS corpus.\n",
    "# spa_unigram_tagger = spa_tagger.uni\n",
    "# print (spa_unigram_tagger.tag(sent1))\n",
    "# # POS tagger traned on bigrams of CESS corpus.\n",
    "# spa_bigram_tagger = spa_tagger.bi\n",
    "# print (spa_bigram_tagger.tag(sent2))\n",
    "# print (spa_bigram_tagger.tag_sents(test_sents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Now lets PoD tag everything\n",
    "# etiquetador=StanfordPOSTagger(tagger,jar)\n",
    "# etiquetas=etiquetador.tag(tokenized_words)\n",
    "# etiquetas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
